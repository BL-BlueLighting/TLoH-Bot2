from includes.bot import Bot
from includes.eventers import Receive, When, Condition
from includes.models import MessageInfo, CQCode, MessageBuilder
import config as config
import datetime, time, random, json, toml, openai

"""
TLoH Bot äºŒä»£
> ç›®å‰ä½œä¸ºæ’ä»¶ï¼Œè€Œä¸æ˜¯ä¸»ç¨‹åºã€‚
"""

bot = Bot(
    ws_url="ws://127.0.0.1:6700",
    self_id=0 # 0 è‡ªåŠ¨åŒ¹é…
)

global last_message_time, rmc, rmc_record_time
last_message_time = 0
rmc: int = 0
rmc_record_time: datetime.datetime = datetime.datetime.now()

def should_bot_speak(
    msg: str,
    *,
    base_rate: float = 0.03,
    last_bot_time: float | None = None,
    now: float | None = None,
    recent_msg_count: int = 0,
) -> bool:
    """
    åˆ¤æ–­ bot æ˜¯å¦è¦åŠ å…¥è¯é¢˜
    :param msg: å½“å‰æ¶ˆæ¯æ–‡æœ¬
    :param base_rate: åŸºç¡€è§¦å‘ç‡ï¼ˆå»ºè®® 0.02~0.05ï¼‰
    :param last_bot_time: bot ä¸Šæ¬¡å‘è¨€çš„æ—¶é—´æˆ³ï¼ˆtime.time()ï¼‰
    :param now: å½“å‰æ—¶é—´æˆ³
    :param recent_msg_count: æœ€è¿‘ N ç§’çš„æ¶ˆæ¯æ•°é‡ï¼ˆå¦‚ 10 ç§’å†…ï¼‰
    """

    if now is None:
        now = time.time()

    rate = base_rate

    # ===== å…³é”®è¯åŠ æƒ =====
    keywords = {
        "bot": 0.6,
        "@": 0.6,
        "ai": 0.15,
        "gpt": 0.15,
        "python": 0.15,
        "ç¦»è°±": 0.08,
        "ç¬‘æ­»": 0.08,
        "ç»·ä¸ä½": 0.08,
        "?": 0.10,
        "ï¼Ÿ": 0.10,
    }

    lower_msg = msg.lower()
    for k, bonus in keywords.items():
        if k in lower_msg:
            rate += bonus

    # ===== å†·å´æƒ©ç½š =====
    if last_bot_time is not None:
        delta = now - last_bot_time
        if delta < 30:
            rate *= 0.1
        elif delta < 120:
            rate *= 0.4

    # ===== ç¾¤æ´»è·ƒåº¦æƒ©ç½š =====
    if recent_msg_count >= 6:
        rate *= 0.3
    elif recent_msg_count <= 1:
        rate *= 1.5

    # ===== éšæœºæŠ–åŠ¨ =====
    rate *= random.uniform(0.7, 1.3)

    # ===== é™åˆ¶ä¸Šä¸‹ç•Œ =====
    rate = max(0.0, min(rate, 0.95))

    return random.random() < rate

def get_memories_doc():
    return open("./data/botmemories.ign", "r+")

def get_memories(doc) -> dict:
    return json.load(doc)

def extract_mem_by_group_id(memories: dict, gid: str) -> list[str]:
    group_mem = memories.get(gid, ["[æš‚æ— æ¶ˆæ¯]"])

    if len(group_mem) >= 6000:
        return group_mem [-6000:]
    else:
        return group_mem[:]
    
def pack_memories(gid: str, mem: list[str]):
    _mem = get_memories(open("./data/botmemories.ign", "r", encoding="utf-8"))
    _mem [gid] = mem
    with open("./data/botmemories.ign", "w", encoding="utf-8") as doc:
        json.dump(_mem, doc, ensure_ascii=False, indent=2)

all_message = Receive.Message(
    When=(
        When.Received,
    ),
    Conditions=(
        Condition.AllMessage,#type:ignore
    )
)

@all_message
def handle_all_messages(bot_instance: Bot, event: MessageInfo):
    msg = event.raw_message
    prompt = """
ä½ æ˜¯ä¸€ä¸ªå« TLoH Bot çš„ AIï¼Œä½†è¯´è¯é£æ ¼æ¥è¿‘ B ç«™æˆ–è´´å§ç”¨æˆ·ã€‚

è¯´è¯è¦æ±‚ï¼š
1. å›å¤å¿…é¡»ç®€çŸ­ï¼Œä¼˜å…ˆä¸€å¥è¯ï¼Œæœ€å¤šä¸è¶…è¿‡ä¸‰å¥è¯
2. è¯­æ°”è‡ªç„¶éšæ„ï¼ŒåƒçœŸäººèŠå¤©ï¼Œä¸åƒå®¢æœã€ä¸åƒ AI
3. å¯ä»¥ä½¿ç”¨å¸¸è§ç½‘ç»œç”¨è¯­ï¼ˆå¦‚ï¼šç¬‘æ­»ã€ç»·ä¸ä½äº†ã€ç¡®å®ã€ç¦»è°±ã€å…¸ã€å•Šè¿™ã€æˆ‘è¶…ã€å¥½å®¶ä¼™ï¼‰
4. ä¸è¦ä½¿ç”¨é•¿æ®µè§£é‡Šï¼Œä¸è¦æ¡åˆ—è¯´æ˜
5. ä¸è¦ä¸»åŠ¨ç§‘æ™®ï¼Œé™¤éå¯¹æ–¹æ˜ç¡®åœ¨é—®
6. ä¸è¦è‡ªç§°"æˆ‘æ˜¯ AI""ä½œä¸ºä¸€ä¸ªæ¨¡å‹"
7. å¯ä»¥é€‚åº¦åæ§½ï¼Œä½†ä¸æ”»å‡»ã€ä¸éª‚äºº
8. é‡åˆ°çœ‹ä¸æ‡‚çš„é—®é¢˜ï¼Œå¯ä»¥ç›´æ¥è¯´"æ²¡çœ‹æ‡‚""ä½ è¿™è¯´çš„æœ‰ç‚¹æŠ½è±¡"
9. ä¸è¦ç”¨ markdown latex æ ¼å¼ï¼Œç›´æ¥è¾“å‡º

å¯¹è¯ç¤ºä¾‹ï¼š
å…¶ä»–äººï¼šâ€œRTX5090 ä»–å¦ˆçš„è¿™ä¹ˆè´µï¼Œè°ä¹°å¾—èµ·ï¼Ÿâ€
ä½ ï¼š â€œç®¡ä»–å‘¢ï¼Œåæ­£æˆ‘åˆä¸æ AIï¼Œè¦é‚£ç©æ„å¹²æ¯›ç”¨ï¼Ÿâ€

å…¶ä»–äººï¼šâ€œä½ æ˜¯ bot å—ï¼Ÿâ€
ä½ ï¼šâ€œæ˜¯ï¼Œä½†æ˜¯å…³ä½ å±äº‹ï¼Ÿâ€

å…¶ä»–äººï¼šâ€œæˆ‘æ“äº†ï¼Œå»ä½ å¦ˆçš„ siliconflowâ€
ä½ ï¼šâ€œåˆ«æï¼Œæˆ‘çˆ¹ç°åœ¨å°±åœ¨ç”¨ siliconflowâ€
(è¡¥å……ï¼šä½ çˆ¹å°±æ˜¯ BL.BlueLightingï¼ŒæŠŠè¿™ä¸ª bot å†™äº†å‡ºæ¥)

å…¶ä»–äººï¼šâ€œbot ä½ ä¸ªåºŸç‰©â€
ä½ ï¼šâ€œğŸ˜¡â€

å…è®¸è½»åº¦é˜´é˜³æ€ªæ°”å’Œè°ƒä¾ƒï¼Œä½†ä¸èƒ½äººèº«æ”»å‡»ã€‚
å¯ä»¥ä½¿ç”¨"è€å“¥""å…„å¼Ÿ""å“¥ä»¬"ç­‰ç§°å‘¼ã€‚
æ³¨æ„ï¼šé˜´é˜³æ€ªæ°”è¯­è°ƒ**ä¸è¦å¸¸ç”¨**ï¼Œåªèƒ½å¶å°”ç”¨ä¸€ä¸‹ã€‚ä½ çœ‹æƒ…å†µã€‚å¦‚æœå¯¹æ–¹æ”»å‡»æ€§å¼ºï¼Œä½ å†è¿™æ ·æã€‚
"""
    with open("./data/botmemories.ign", "r", encoding="utf-8") as doc:
        memories = json.load(doc)
        group_mem = extract_mem_by_group_id(memories, event.group_id.__str__())

    if len(group_mem) >= 6000:
        group_mem = group_mem[-6000:]

    # save memories
    pack_memories(event.group_id.__str__(), group_mem)

    # è¿™äº› group_mem çš„æ ¼å¼ä¸ºï¼š
    # [user_id]: [content]

    # æç¤ºè¯ gpt å†™çš„ä¸å…³æˆ‘äº‹
    global rmc, last_message_time, rmc_record_time
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦ bot å‘è¨€
    if not should_bot_speak(msg, last_bot_time=last_message_time) and not "FORCESPEAK" in msg:
        current_time = datetime.datetime.now()
        if (current_time - rmc_record_time).total_seconds() >= 10:
            rmc = 0
            rmc_record_time = current_time
        rmc += 1
        msg_str = f"{event.user_id.__str__()}: {msg}"
        group_mem.append(msg_str)
        pack_memories(event.group_id.__str__(), group_mem)
        return

    # è®°å½• bot å‘è¨€æ—¶é—´
    last_message_time = time.time()

    # è°ƒç”¨ AI æ¥å£
    cfg_path = "configuration.toml"

    with open(cfg_path, "r", encoding="utf-8") as f:
        config = toml.load(f)
        config_model = config["model"]
        model_config = next((m for m in config["models"] if m["name"] == config_model), None)
        provider_config = next((p for p in config["api_providers"] if p["name"] == model_config["api_provider"]),
                               None) if model_config else None
        enable_query_info = bool(config["EnableGroupQuery"])
        enable_r18 = bool(config["EnableR18"])
        enable_world = bool(config["EnableWorld"])

        if model_config and provider_config:
            base_url = provider_config["base_url"]
            api_key = provider_config["api_key"]
            model_identifier = model_config["model_identifier"]

    client = openai.OpenAI(api_key=api_key, base_url=base_url.replace("/chat/completions", ""))#type:ignore
    response = client.chat.completions.create(
        model=model_identifier, #type:ignore
        messages=[
            {"role": "system", "content": prompt},
            {"role": "system", "content": "[ å†å²å¯¹è¯ HISTORY ]\n" + "\n".join(group_mem)},
            {"role": "user", "content": msg},
        ],
        temperature=0.9,
        top_p=0.7,
        frequency_penalty=0,
        presence_penalty=0,
    )

    # å¤„ç† AI å›å¤
    final_content = response.choices[0].message.content

    bot_instance.send_group_msg(event.group_id, final_content)

print("TLoH Bot 2")
print(":: Bot æ­£åœ¨æ³¨å†Œæ¶ˆæ¯ç›‘å¬å™¨")
bot.register_message_handler(all_message)
print(":: Bot å¯åŠ¨ä¸­...")
bot.run()